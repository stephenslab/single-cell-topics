---
title: "DE analysis using a topic model: evaluation using simulated data"
author: Peter Carbonetto
output: workflowr::wflow_html
---

Here we evaluate the new topic-model-based DE methods implemented in
the fastTopics package using simulated data. We show the benefits of
shrinking the estimates using adaptive shrinkage and quantifying
uncertainty. We also compare with DESeq2 and show that the results are
very similar when the cells separate into distinct clusters. In this
first analysis, we focus on the case of $K = 2$ topics, and in the
[second analysis](de_analysis_detailed_look_more.html) we evaluate the
new DE methods with more than two topics.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.show = "hold",fig.align = "center",dpi = 120)
```

Load the packages used in the analysis below, and some additional
functions for simulating the data.

```{r load-pkgs, message=FALSE, warning=FALSE}
library(Matrix)
library(scran)
library(DESeq2)
library(Seurat)
library(MAST)
library(fastTopics)
library(ggplot2)
library(cowplot)
source("../code/de_analysis_functions.R")
```

Simulate UMI counts
-------------------

We begin by simulating counts from a rank-2 Poisson NMF model with
parameters chosen to roughly mimic the UMI counts from a single-cell
RNA sequencing experiment. For now, we focus our attention on the
rank-2 (*i.e.*, two topics) case to simplify the evaluation of the DE
methods; comparing gene expression between three or topics brings some
additional complications which aren't necessary for assessing basic
properties of the new DE methods. For more details on how the data are
simulated, see function `simulate_twotopic_umi_data`.

```{r simulate-counts}
set.seed(1)
dat <- simulate_twotopic_umi_data()
X <- dat$X
F <- dat$F
L <- dat$L
```

Before fitting a topic model and running a DE analysis, we first
inspect some basic properties of the simulated data.

The sample sizes (total counts for each cell) are roughly normal on
the (base-10) log scale:

```{r sizes, fig.height=2, fig.width=3.5}
s <- rowSums(X)
ggplot(data.frame(s = log10(s)),aes(x = s)) +
  geom_histogram(color = "white",fill = "black",bins = 32) +
  labs(x = "log10 size",y = "cells") +
  theme_cowplot()
```

The expression rates were simulated so that they are normally
distributed on the log scale:

```{r gene-expression-rates, fig.height=2, fig.width=3.5}
ggplot(data.frame(f = as.vector(F)),aes(x = log10(f))) +
  geom_histogram(color = "white",fill = "black",bins = 32) +
  labs(x = "log10 expression rate",y = "genes") +
  theme_cowplot()
```

About half of the genes have nonzero differences in expression between
the two topics. Among the nonzero gene expression differences, the
log-fold changes (LFCs) were simulated from the normal distribution:

```{r lfc-density, fig.height=2, fig.width=3.5}
nonzero_lfc <- abs(F[,1] - F[,2]) > 1e-8
lfc <- log2(F[,2]/F[,1])
ggplot(data.frame(lfc = lfc[nonzero_lfc]),aes(x = lfc)) +
  geom_histogram(color = "white",fill = "black",bins = 32) +
  labs(x = "LFC",y = "genes") +
  theme_cowplot()
mean(nonzero_lfc)
```

Fit multinomial topic model to UMI counts
-----------------------------------------

Now we fit a multinomial topic model to the simulated UMI count data.
To simplify evaluation, we assume that the topic proportions are
known, and fix them to their ground-truth values. In this way, the
only error that can arise is in the estimates of the expression rates
$f_{ij}$.

```{r fit-poisson-nmf, warning=FALSE, message=FALSE, cache=TRUE}
fit0 <- init_poisson_nmf(X,F = dat$F,L = with(dat,s*L))
fit <- fit_poisson_nmf(X,fit0 = fit0,numiter = 40,method = "scd",
                       update.loadings = NULL,verbose = "none")
fit <- poisson2multinom(fit)
summary(fit)
```

DE analysis with and without shrinkage
--------------------------------------

We perform a DE analysis *without* shrinking the LFC estimates,

```{r de-noshrink, message=FALSE, cache=TRUE}
set.seed(1)
de.noshrink <- de_analysis(fit,X,shrink.method = "none",
                           control = list(ns = 10000,nc = 2))
```

and we perform a second DE analysis using adaptive shrinkage to
shrink (and hopefully improve accuracy of) the LFC estimates,

```{r de-with-ash, message=FALSE, cache=TRUE}
set.seed(1)
de <- de_analysis(fit,X,shrink.method = "ash",control = list(ns = 1e4,nc = 2))
```

This next scatterplot compares the posterior mean LFC estimates with
and without performing the shrinkage step. As expected, the smaller
(in magnitude) LFC estimates and the LFC estimates corresponding to genes
with lower expression tend to be more strongly shrunk toward zero.

```{r shrink-vs-noshrink-scatterplot, fig.height=2.75, fig.width=5}
pdat <- data.frame(noshrink  = de.noshrink$postmean[,2],
                   shrink    = de$postmean[,2],
                   log10mean = log10(de$f0))
ggplot(pdat,aes(x = noshrink,y = shrink,fill = log10mean)) +
  geom_point(shape = 21,color = "white",size = 2) +
  geom_abline(intercept = 0,slope = 1,color = "black",linetype = "dotted") +
  scale_fill_gradient2(low = "deepskyblue",mid = "gold",high = "orangered",
                       midpoint = -4) +
  labs(x = "original LFC estimates",
       y = "shrunken LFC estimates") +
  theme_cowplot()
```

The next two bar charts show the overall impact of the shrinkage on
the distribution of the *z*-scores; dark blue bars are for
diifferentially expressed genes, and orange bars are for
non-differentially expressed genes. (For better visualizataion the
*z*-scores have been projected onto the interval $[-8,+8]$.)

```{r shrink-vs-noshrink-zscores, fig.height=2.75, fig.width=8, warning=FALSE}
pdat <- data.frame(noshrink = clamp(de.noshrink$z[,2],-8,+8),
                   shrink   = clamp(de$z[,2],-8,+8),
                   de       = factor(nonzero_lfc))
p1 <- ggplot(pdat,aes(x = noshrink,color = de,fill = de)) +
  geom_histogram(bins = 64) +
  scale_color_manual(values = c("darkorange","darkblue")) +
  scale_fill_manual(values = c("darkorange","darkblue")) +
  labs(x = "z-score",y = "genes",title = "without shrinkage") +
  theme_cowplot()
p2 <- ggplot(pdat,aes(x = shrink,color = de,fill = de)) +
  geom_histogram(bins = 64) +
  scale_color_manual(values = c("darkorange","darkblue")) +
  scale_fill_manual(values = c("darkorange","darkblue")) +
  labs(x = "z-score",y = "genes",title = "with shrinkage") +
  theme_cowplot()
print(plot_grid(p1,p2))
```

And the next two bar charts show the overall impact of the shrinkage
on the *p*-values.

```{r shrink-vs-noshrink-pvalues, fig.height=2.75, fig.width=8, warning=FALSE}
pdat <- data.frame(noshrink = 10^(-de.noshrink$lpval[,2]),
                   shrink   = 10^(-de$lpval[,2]),
                   de       = factor(nonzero_lfc))
p1 <- ggplot(pdat,aes(x = noshrink,color = de,fill = de)) +
  geom_histogram(bins = 64) +
  scale_color_manual(values = c("darkorange","darkblue")) +
  scale_fill_manual(values = c("darkorange","darkblue")) +
  labs(x = "p-value",y = "genes",title = "without shrinkage") +
  theme_cowplot()
p2 <- ggplot(pdat,aes(x = shrink,color = de,fill = de)) +
  geom_histogram(bins = 64) +
  scale_color_manual(values = c("darkorange","darkblue")) +
  scale_fill_manual(values = c("darkorange","darkblue")) +
  labs(x = "p-value",y = "genes",title = "with shrinkage") +
  theme_cowplot()
print(plot_grid(p1,p2))
```

Both the *z*-score and *p*-value plots show that the adaptive
shrinkage step more strongly encourages the LFC estimates for the
non-DE genes toward zero.

Quantifying uncertainty in the LFC estimates
--------------------------------------------

Above, we assessed the usefulness of applying the adaptive shrinkage
method to improve the accuracy of the LFC estimates. This idea of
shrinking the LFC estimates is not new, and is already implemented in
DESeq2 (also using adaptive shrinkage).

A more important aspect of the DE analysis methods in fastTopics is
that the methods also quantify uncertainty in the LFC estimates by
computing posterior HPD intervals. To see why this is important,
consider this plot comparing the MLEs of the LFCs against the shrunken
posterior mean estimates of the LFCs. What is striking is that even
very large (in magnitude) point estimates are sometimes shrunk to
zero; this happens because although the MLEs can be far from zero, the
HPD intervals are also very large.

```{r point-vs-shrunken-scatterplot, fig.width=8, fig.height=2.5}
pdat <- data.frame(est       = de$est[,2],
                   shrink    = de$postmean[,2],
				   log10mean = log10(de$f0))
ggplot(pdat,aes(x = est,y = shrink,fill = log10mean)) +
  geom_point(shape = 21,color = "white",size = 2) +
  geom_abline(intercept = 0,slope = 1,color = "black",linetype = "dotted") +
  scale_fill_gradient2(low = "deepskyblue",mid = "gold",high = "orangered",
                       midpoint = -4) +
  labs(x = "point estimates",
       y = "shrunken estimates") +
  theme_cowplot()
```

To illustrate the impact of this on the DE analysis, consider the K-L
divergence measure used in Dey, Hsiao & Stephens (2017) to rank genes,
a measure which doesn't fully account for uncertainty in the
LFCs. Since the K-L divergence is not a signed ranking measure like
the *z*-score, here we focus only on genes that are estimated to have
greater expression in topic 2 compared to topic 1. (For better
visualization, we show the distribution of the K-L divergences on the
log scale, and any *z*-scores larger than 4 are shown as 4.)

```{r kl-vs-fasttopics-1, fig.width=8, fig.height=2.5}
i     <- which(fit$F[,2] - fit$F[,1] > -1e-8)
D     <- fastTopics:::min_kl_poisson(fit$F)
pdat1 <- data.frame(lkl = log10(D[,2] + 1e-8),de = factor(nonzero_lfc))
pdat2 <- data.frame(z = clamp(de$z[,2],-4,+4),de = factor(nonzero_lfc))
pdat1 <- pdat1[i,]
pdat2 <- pdat2[i,]
p1 <- ggplot(pdat1,aes(x = lkl,color = de,fill = de)) +
  geom_histogram(bins = 64) +
  scale_color_manual(values = c("darkorange","darkblue")) +
  scale_fill_manual(values = c("darkorange","darkblue")) +
  labs(x = "log10 K-L divergence",y = "genes") +
  theme_cowplot()
p2 <- ggplot(pdat2,aes(x = z,color = de,fill = de)) +
  geom_histogram(bins = 64) +
  scale_color_manual(values = c("darkorange","darkblue")) +
  scale_fill_manual(values = c("darkorange","darkblue")) +
  labs(x = "z-score",y = "genes") +
  theme_cowplot()
print(plot_grid(p1,p2))
```

Without fully accounting for uncertainty, sometimes genes with very
large LFCs get highly ranked by the K-L divergence measure (while also
being uncertain about these estimates), whereas this happens less
often for the *z*-scores.

To underscore the importance of accounting for uncertainty, suppose we
used the KL-divergences and the *p*-values returned by
`de_analysis(shrink.method = "ash")` to discover differentially
expressed genes. The power vs. FDR curves show that, at low false
discovery rates (say, FDR < 0.2), the rankings accounting for
uncertainty in the LFC estimates (with or without shrinkage) perform
much better than the ranking based on the KL divergence measure. We
also see that using adaptive shrinkage produces a modest improvement.
As before, to draw a fair comparison with the K-L
divergence-based ranking, we focus only on genes that are estimated to
have greater expression in topic 2 compared to topic 1.

```{r kl-vs-fasttopics-2, fig.width=4.25, fig.height=3}
pdat1 <- create_fdr_vs_power_curve(-D[i,2],nonzero_lfc[i],length.out = 140)
pdat2 <- create_fdr_vs_power_curve(-de.noshrink$lpval[i,2],nonzero_lfc[i],
                                   length.out = 140)
pdat3 <- create_fdr_vs_power_curve(-de$lpval[i,2],nonzero_lfc[i],
                                   length.out = 140)
pdat  <- rbind(cbind(pdat1,method = "kl"),
               cbind(pdat2,method = "noshrink"),
               cbind(pdat3,method = "shrink"))
ggplot(pdat,aes(x = fdr,y = power,color = method)) +
  geom_line(size = 0.65,orientation = "y")  +
  scale_color_manual(values = c("royalblue","limegreen","darkorange")) +
  theme_cowplot()
```

For a different perspective we also show the ROC curve comparing the
true positive rate (TPR) against the false positive rate (FPR):

```{r kl-vs-fasttopics-3, fig.width=4.5, fig.height=3}
pdat1 <- create_roc_curve(-D[i,2],nonzero_lfc[i],length.out = 100)
pdat2 <- create_roc_curve(-de.noshrink$lpval[i,2],nonzero_lfc[i],
                          length.out = 100)
pdat3 <- create_roc_curve(-de$lpval[i,2],nonzero_lfc[i],length.out = 100)
pdat  <- rbind(cbind(pdat1,method = "kl"),
               cbind(pdat2,method = "noshrink"),
               cbind(pdat3,method = "shrink"))
ggplot(pdat,aes(x = fpr,y = tpr,color = method)) +
  geom_line(size = 0.65) +
  geom_abline(intercept = 0,slope = 1,color = "black",linetype = "dotted") +
  scale_color_manual(values = c("royalblue","limegreen","darkorange")) +
  theme_cowplot()
```

Assessing accuracy of the Monte Carlo estimates
-----------------------------------------------

We use a simple MCMC algorithm to estimate LFC posterior statistics.
To assess accuracy of the posterior calculations, we perform the DE
analysis twice, and compare the *z*-scores returned by the two
`de_analysis` runs. If the *z*-scores are very consistent between the
two runs this suggests that they are mostly accurate.

To illustrate, we first compute the posteriors using 1,000 Monte Carlo
samples:

```{r assess-monte-carlo-1}
set.seed(1); de1 <- de_analysis(fit,X,control = list(ns = 1000,nc = 2))
set.seed(2); de2 <- de_analysis(fit,X,control = list(ns = 1000,nc = 2))
```

Now we compare the *z*-scores estimated by these two runs in a simple
scatterplot:

```{r assess-monte-carlo-2, fig.width=4, fig.height=3}
pdat <- data.frame(x = de1$z[,2],y = de2$z[,2])
ggplot(pdat,aes(x = x,y = y)) +
  geom_point(shape = 21,color = "white",fill = "royalblue",size = 2) +
  geom_abline(intercept = 0,slope = 1,color = "black",linetype = "dotted") +
  labs(x = "first z-score estimate",
       y = "second z-score estimate") +
  theme_cowplot()
```

Here we see that the concordance is quite strong, but we should be
able to improve the accuracy by running a longer Markov chain. Let's
try simulating a longer Markov chain with 10,000 iterations:

```{r assess-monte-carlo-3, cache=TRUE}
set.seed(1); de1 <- de_analysis(fit,X,control = list(ns = 10000,nc = 2))
set.seed(2); de2 <- de_analysis(fit,X,control = list(ns = 10000,nc = 2))
```

Indeed, with 10,000 Monte Carlo samples the two sets of *z*-score
estimates are now very similar:

```{r assess-monte-carlo-4, fig.width=3.25, fig.height=3}
pdat <- data.frame(x = de1$z[,2],y = de2$z[,2])
ggplot(pdat,aes(x = x,y = y)) +
  geom_point(shape = 21,color = "white",fill = "royalblue",size = 2) +
  geom_abline(intercept = 0,slope = 1,color = "black",linetype = "dotted") +
  labs(x = "first z-score estimate",
       y = "second z-score estimate") +
  theme_cowplot()
```

Comparison with DESeq2 and MAST
-------------------------------

DESeq2 and MAST are leading methods for DE analysis in single-cell RNA
sequencing data (they are two of the main DE analysis methods
supported by [Seurat][seurat-de-vignette]). Here we apply DESeq2 and
MAST to these same UMI count data. We show in particular that the
fastTopics DE results are very similar to DESeq2.

Since DESeq2 and MAST assume discrete cell types or conditions, we
apply DESeq2 and MAST to the subset of cells that were are almost
entirely (>99%) simulated from one topic; in this simulation,
approximatmely 2/3 of the cells were simulated from one topic
(entirely topic 1 or entirely topic 2), and the remaining 1/3 were
simulated from a mixture of the two topics (with the mixture
proportion being uniformly distributed on $[0,1]$). There may be a
small loss of accuracy in the DESeq2 results since DESeq2 is using
only 2/3 of the data, but overall we expect that the fastTopics DE
analysis should closely replicate the DESeq2 analysis. By contast,
MAST performs substantially worse than both DESeq2 and fastTopics,
although has the benefit of being much faster for large single-cell
data sets.

```{r deseq-1}
single_topic <- apply(L,1,max) > 0.99
mean(single_topic)
```

These next few lines of code prepare the UMI count data for analysis
with DESeq2.

```{r deseq-2}
i <- which(single_topic)
Y <- t(X[i,])
cluster <- factor(apply(L,1,which.max))
deseq <- DESeqDataSetFromMatrix(Y,data.frame(cluster = cluster[i]),~cluster)
```

Now we perform the DE analysis using DESeq2, using the settings
recommended for single-cell RNA-seq data (see the main DESeq2 vignette
for details). To replicate the fastTopics analysis as closely as
possible, we also shrink the LFC estimates using adaptive shrinkage.

```{r deseq-3}
sizeFactors(deseq) <- calculateSumFactors(Y)
deseq <- DESeq(deseq,test = "LRT",reduced=~1,useT = TRUE,minmu = 1e-6,
               minReplicatesForReplace = Inf)
deseq <- lfcShrink(deseq,coef = "cluster_2_vs_1",type = "ashr")
```

Now we run the MAST differential expression analysis via the
"FindMarkers" interface in Seurat:

```{r mast, warning=FALSE}
seurat <- CreateSeuratObject(counts = t(X))
Idents(seurat) <- cluster
mast <- FindMarkers(seurat,ident.1 = "2",ident.2 = NULL,test.use = "MAST",
                    logfc.threshold = 0,min.pct = 0)
```

The posterior mean estimates from the fastTopics DE analysis closely
align with the DESeq2 estimates:

```{r fasttopics-vs-deseq2-lfc, fig.height=3, fig.width=3}
pdat <- data.frame(DESeq2     = deseq$log2FoldChange,
                   fastTopics = de$postmean[,2])
ggplot(pdat,aes(x = DESeq2,y = fastTopics)) +
  geom_point() +
  geom_abline(intercept = 0,slope = 1,color = "skyblue",linetype = "dotted") +
  ggtitle("LFC estimates") +
  theme_cowplot()
```

And likewise the *z*-scores:

```{r fasttopics-vs-deseq2-zscores, fig.height=3, fig.width=3}
deseq$z <- with(deseq,log2FoldChange/lfcSE)
pdat <- data.frame(DESeq2     = deseq$z,
                   fastTopics = de$z[,2])
ggplot(pdat,aes(x = DESeq2,y = fastTopics)) +
  geom_point() +
  geom_abline(intercept = 0,slope = 1,color = "skyblue",linetype = "dotted") +
  ggtitle("z-scores") +
  theme_cowplot()
```

The *p*-value and LFSR distributions are also correspondingly similar
in shape:

```{r fasttopics-vs-deseq2-pvalues, fig.height=2.5, fig.width=8, warning=FALSE}
pdat <- data.frame(deseq2 = deseq$padj,
                   shrink = de$lfsr[,2],
                   de     = factor(nonzero_lfc))
p1 <- ggplot(pdat,aes(x = deseq2,color = de,fill = de)) +
  geom_histogram(bins = 64) +
  scale_color_manual(values = c("darkorange","darkblue")) +
  scale_fill_manual(values = c("darkorange","darkblue")) +
  labs(x = "p-value",y = "genes",title = "DESeq2") +
  theme_cowplot()
p2 <- ggplot(pdat,aes(x = shrink,color = de,fill = de)) +
  geom_histogram(bins = 64) +
  scale_color_manual(values = c("darkorange","darkblue")) +
  scale_fill_manual(values = c("darkorange","darkblue")) +
  labs(x = "lfsr",y = "genes",title = "fastTopics") +
  theme_cowplot()
print(plot_grid(p1,p2))
```

The MAST LFC estimates broadly resemble the fastTopics estimates,
although the MAST estimates tend to be underestimates, and
occasionally some estimates that are much more extreme (and probably
would benefit from a shrinkage step, e.g., using adaptive shrinkage):

```{r fasttopics-vs-mast, fig.height=3, fig.width=3}
genes <- rownames(mast)
pdat  <- data.frame(MAST       = mast$avg_logFC,
                    fastTopics = de$postmean[genes,2])
ggplot(pdat,aes(x = MAST,y = fastTopics)) +
  geom_point() +
  geom_abline(intercept = 0,slope = 1,color = "skyblue",linetype = "dotted") +
  ggtitle("LFC estimates") +
  theme_cowplot()
```

The `FindMarkers` function in Seurat does not output *z*-scores, but
we can compare *p*-values:

```{r fasttopics-vs-mast-pvalues, fig.height=3, fig.width=3}
pdat <- data.frame(MAST       = pmax(-10,log10(mast$p_val)),
                   fastTopics = pmax(-10,-de$lpval[genes,2]))
ggplot(pdat,aes(x = MAST,y = fastTopics)) +
  geom_point() +
  geom_abline(intercept = 0,slope = 1,color = "skyblue",linetype = "dotted") +
  ggtitle("log10 p-values") +
  theme_cowplot()
```

Again, the *p*-values are broadly similar. The effect of the
fastTopics shrinkage step is apparent as many less significant
*p*-values get driven toward 1.

Based on these results, we would expect that the performance of
fastTopics and DESeq2 for discovering differentially expressed genes
is very similar, with a slight advantage given to fastTopics (although
consider it is using 1/3 more data):

```{r fdr-vs-power, fig.height=3, fig.width=4.5}
mast_pval                 <- rep(1,10000)
names(mast_pval)          <- colnames(X)
mast_pval[rownames(mast)] <- mast$p_val
pdat1 <- create_fdr_vs_power_curve(deseq$padj,nonzero_lfc,length.out = 200)
pdat2 <- create_fdr_vs_power_curve(de$lfsr[,2],nonzero_lfc,length.out = 200)
pdat3 <- create_fdr_vs_power_curve(mast_pval,nonzero_lfc,length.out = 200)
pdat  <- rbind(cbind(pdat1,method = "DESeq2"),
               cbind(pdat2,method = "fastTopics"),
               cbind(pdat3,method = "MAST"))
ggplot(pdat,aes(x = fdr,y = power,color = method)) +
  geom_line(size = 0.65,orientation = "y")  +
  scale_color_manual(values = c("darkblue","tomato","dodgerblue")) +
  theme_cowplot()
```

Interestingly, MAST underperforms DESeq2 at lower FDRs, and
overperforms DESeq2 at higher FDRs, whereas fastTopics performs at
least as well as the other methods at any FDR.

And here is the ROC curve comparing DESeq2, MAST and fastTopics:

```{r roc, fig.height=3, fig.width=4.5}
pdat1 <- create_roc_curve(deseq$padj,nonzero_lfc,length.out = 100)
pdat2 <- create_roc_curve(de$lfsr[,2],nonzero_lfc,length.out = 100)
pdat3 <- create_roc_curve(mast_pval,nonzero_lfc,length.out = 100)
pdat  <- rbind(cbind(pdat1,method = "DESeq2"),
               cbind(pdat2,method = "fastTopics"),
               cbind(pdat3,method = "MAST"))
ggplot(pdat,aes(x = fpr,y = tpr,color = method)) +
  geom_line(size = 0.65)  +
  geom_abline(intercept = 0,slope = 1,color = "black",linetype = "dotted") +
  scale_color_manual(values = c("darkblue","tomato","dodgerblue")) +
  theme_cowplot()
```

[seurat-de-vignette]: https://satijalab.org/seurat/articles/de_vignette.html
